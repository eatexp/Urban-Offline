Briefing on the Offline-First Emergency Intelligence Application

Executive Summary

This briefing outlines the architecture and strategic validation for a proposed offline-first emergency preparedness mobile application. The analysis confirms the project's overall feasibility, with technical specifications aligning with real-world benchmarks at an estimated 90% accuracy. The core design is a robust, three-layer hybrid intelligence model prioritizing low-latency, on-device processing and absolute reliability in network-denied environments. The architecture is built to operate within a strict 500MB total application size budget, ensuring accessibility for users with limited data or storage.

Key findings necessitate several strategic pivots. A critical dealbreaker is the licensing for UK National Health Service (NHS) content, which contains geographic restrictions incompatible with the app's global, "airgapped" value proposition. The recommended path is to pivot to alternative, permissively licensed sources like WikiProject Medicine and OpenWHO. The on-device AI strategy has been refined to prioritize Apple's highly efficient MLTextClassifier framework (3-10ms latency, 1-10MB footprint) for core intent classification, rejecting larger, license-restricted models like MobileLLM, which exceed the size budget.

The proposed 400MB MVP budget is validated as achievable, allocating approximately 150-200MB for mapping data, 150MB for medical/legal content, and 50MB for code and models, leaving a healthy buffer for future growth. Recommended next steps focus on rapid, low-risk prototyping to validate key assumptions, including an "MLTextClassifier spike" to confirm its user experience viability and a "one-week sprint" to author and test a complete conversational triage script using the Ink narrative engine. With these adjustments, the project is considered production-ready and poised for a successful 3-6 month MVP development cycle.

1. Project Overview and Feasibility

The project outlines a sophisticated emergency preparedness application designed around an "offline-first" philosophy. This principle ensures that all core functionality—from triage and first aid guidance to mapping and legal information—is available without an internet connection, addressing the critical reality of network failures during emergencies.

1.1. Core Functional Requirements

The application is designed to deliver a comprehensive suite of emergency tools:

* Natural Language Understanding: Classify user intent from natural language queries in emergency scenarios.
* Conversational Guidance: Provide step-by-step guidance through branching conversation trees for critical triage situations.
* Information Retrieval: Enable hybrid full-text and semantic search over an extensive offline library of medical, legal, and survival content.
* Offline Mapping: Offer offline maps with awareness of emergency-critical locations.
* Jurisdiction-Aware Content: Deliver content tailored to specific regions, with a primary focus on the UK and fallbacks for international use.

1.2. Overall Validation and Production Readiness

The technical specifications present a feasible and production-ready blueprint. The selection of mature technologies—such as SQLite for the database, the Ink narrative engine for conversations, and MapLibre for mapping—provides a solid foundation. The architecture's alignment with proven patterns from successful apps by the Red Cross and Kiwix further validates its design choices. The project is poised for a successful MVP development cycle with a 2-3 person engineering team over a 3-6 month timeline, beginning with an iOS-first approach.

2. Core Architecture: The Three-Layer Hybrid Intelligence Model

The application's architecture is an innovative three-layer model designed to balance speed, deterministic accuracy, and depth of information retrieval. This hybrid approach avoids the risks of AI "hallucination" in high-stakes scenarios while providing intelligent, context-aware assistance.

* Layer 1: Intent Classification: A lightweight, on-device machine learning model instantly classifies the user's natural language query into a predefined emergency category. This acts as a rapid, intelligent router.
* Layer 2: Narrative Engine: For high-stakes scenarios (e.g., cardiac arrest, severe bleeding), the app executes a pre-authored, deterministic conversation tree. This provides verifiable, step-by-step guidance, eliminating the risk of incorrect AI-generated advice.
* Layer 3: Knowledge Engine: For less urgent or more complex informational queries, the app falls back to a powerful offline search engine. This engine uses a hybrid of keyword and semantic search to retrieve relevant articles from the app's comprehensive knowledge base.

This model ensures that the most critical, time-sensitive interactions are handled by a reliable, script-based system, while still offering the flexibility and depth of an intelligent search engine for general-purpose queries.

3. On-Device Artificial Intelligence Strategy

The AI strategy is centered on using lightweight, highly optimized, on-device models to provide fast and reliable intelligence without compromising the strict 500MB size budget or draining the battery.

3.1. Recommended AI Models

Primary Model: Apple's MLTextClassifier

For the primary task of intent classification (Layer 1), the specification validates the use of Apple's native NaturalLanguage framework and MLTextClassifier. This choice is driven by its exceptional on-device performance characteristics.

Metric	Performance	Notes
Latency	3–10 milliseconds	Nearly instantaneous, crucial for a responsive conversational UI.
Model Size	1–10 MB	Fits easily within the budget allocated for code and ML assets.
Accuracy	>99%	Achievable on a well-defined set of 20-50 emergency intents with CreateML.
Power Consumption	0.072W (sustained) to 0.454W (peak)	Negligible battery impact; 1,000 inferences consume only 1-2 mAh.

Fallback Model: Distilled & Quantized Transformers

For more complex semantic search and retrieval tasks (Layer 3), a more sophisticated model is required. The recommended approach is to use a distilled and quantized Transformer model.

* Knowledge Distillation: This technique trains a small "student" model (e.g., DistilBERT) to mimic a large "teacher" model, transferring its nuanced understanding into a mobile-friendly format.
* Fine-Tuning: A key opportunity is to fine-tune a model using the specialized FirstAidQA dataset. This dataset contains 5,500 high-quality, validated question-answer pairs derived from a certified first aid manual, enabling the creation of a domain-specific expert model.
* Performance Example: A project fine-tuning a Gemma-3n model on over 86,000 medical examples achieved a 71.5% accuracy rate, demonstrating the viability of creating capable, offline medical assistants.

3.2. Rejected Alternatives

The initial consideration of a 350M parameter MobileLLM has been definitively rejected. Updated analysis shows that even after aggressive INT4 quantization, the model would be approximately 590MB, exceeding the entire app budget on its own. Furthermore, its FAIR license includes non-commercial restrictions, limiting future distribution strategies. Other large models like Gemma-2B, GPT, or Gemini are unsuitable due to their size and reliance on cloud infrastructure.

3.3. Model Optimization Techniques

To fit capable models within the tight constraints, several optimization techniques are essential. The Hugging Face Optimum library is identified as a key tool for streamlining this process.

Technique	Description	Impact
Quantization	Reduces the numerical precision of model weights (e.g., from 32-bit float to 8-bit integer).	Shrinks model size by up to 75% (4x reduction) and accelerates inference on mobile hardware. Both dynamic and static quantization methods are viable.
Knowledge Distillation	Transfers knowledge from a large "teacher" model to a smaller "student" model.	Enables the creation of compact models that retain the advanced capabilities of their larger counterparts, essential for high-quality on-device AI.

3.4. Cross-Platform Parity

To ensure a consistent user experience, the Android version of the app will use the ONNX Runtime Mobile library with the NNAPI delegate. This allows the same optimized model to be deployed on both platforms and enables hardware acceleration on Android devices by offloading computation to specialized hardware like DSPs, GPUs, or NPUs, achieving performance comparable to Apple's CoreML and Neural Engine.

4. Content Strategy and Data Sourcing

The app's utility depends on a comprehensive, authoritative, and legally sound content library. The strategy focuses on sourcing data from permissively licensed repositories to enable global, offline redistribution.

4.1. Critical Decision Point: NHS Content Licensing

A pivotal finding is that the use of UK National Health Service (NHS) content is a dealbreaker. The NHS Syndication License imposes strict geographic restrictions, requiring verification that users are in the UK. This directly conflicts with the app's "download once, use anywhere" value proposition for international travelers and users in areas without connectivity for verification.

Recommendation: Pivot away from NHS content entirely and adopt alternative sources.

4.2. Recommended Content Sources

Content Type	Primary Source	License	Est. Size	Notes
Medical Reference	wikipedia_en_medicine_nopic (ZIM File)	CC-BY-SA	200–250 MB	Comprehensive medical articles from Wikipedia's WikiProject Medicine. Highly compressed for offline use.
Medical Protocols	OpenWHO Guidelines & CDC Protocols	CC-BY-NC / Public Domain	<20 MB	Authoritative guidelines for specific emergency protocols and general first aid.
Geospatial Data	OpenStreetMap (OSM) via OpenMapTiles	ODbL	150–300 MB	Vector tiles for the UK (zoom levels 0-14). Open license avoids restrictions of proprietary services like Mapbox.
UK Legal Content	legislation.gov.uk	Open Government Licence (OGL)	10–20 MB	Key legislation such as the Civil Contingencies Act and PACE codes, made fully searchable offline.
UK Flood Data	Environment Agency	Open Government Licence (OGL)	~15-20 MB	Static flood area data for offline risk assessment.

4.3. Licensing Compliance

Strict adherence to all open-source licenses (ODbL, CC-BY-SA, OGL) is non-negotiable. This requires providing proper attribution for all data sources within the application, likely via a dedicated "Attribution Manifest" page. Failure to comply would terminate the license and pose a significant legal risk.

5. Offline Data Architecture and Storage

The architecture is designed to store, compress, and retrieve hundreds of megabytes of data efficiently on a mobile device.

5.1. Core Database: SQLite with Advanced Extensions

SQLite is the validated choice for the core database due to its reliability, lightweight nature, and ubiquitous support on mobile platforms. Its capabilities are enhanced with specialized extensions to power the Layer 3 Knowledge Engine.

* Full-Text Search (FTS5): Enables fast, efficient keyword-based search. A key optimization is the use of a contentless FTS5 table (detail=none), which reduces the search index size to just 20-30% of the original text size.
* Semantic Search (sqlite-vec): This extension enables the storage and querying of high-dimensional vector embeddings directly within SQLite. This allows the app to perform K-Nearest Neighbors (KNN) searches to find content that is semantically similar to a user's query.
* Hybrid Search: The architecture combines FTS5 and sqlite-vec to provide a superior search experience. A query is run through both engines, and the results are combined (e.g., using Reciprocal Rank Fusion) to rank documents that have both high keyword and semantic relevance.

5.2. Data Compression and Optimization

Aggressive data compression is essential to stay within the 500MB budget.

* Zstandard (zstd) with Dictionaries: This is the recommended approach for text content. By training custom dictionaries on samples of the medical and legal text, compression ratios of 50:1 or higher can be achieved. A 100MB text file could be reduced to just 2MB.
* FlatBuffers: For structured, read-only data like conversation tree indexes, FlatBuffers provides zero-copy access. Benchmarks show it is significantly faster (~77ns access time) than deserializing JSON (~500ns), ensuring a responsive UI.
* Vector Quantization: To manage the size of the semantic search index, binary quantization is recommended. This technique can reduce a vector index from ~150MB down to ~4.8MB—a 32x reduction—with negligible impact on retrieval accuracy.

5.3. Platform-Specific Storage and Delivery

Platform	Storage Location	Content Delivery Mechanism
iOS	Application Support directory. This is critical as the Caches directory can be purged by the OS under storage pressure, which would delete the app's essential offline data. Files must be flagged to be excluded from iCloud backups.	Background Assets Framework (iOS 16.1+). Allows essential assets to be downloaded immediately after installation, often before the app's first launch, ensuring core functionality is instantly available.
Android	Internal Storage managed by the system.	Play Asset Delivery (PAD). Replaces legacy OBB files. It offers flexible modes like install-time for essential data, fast-follow (downloads automatically post-install), and on-demand for optional content packs.

6. Conversational Interface and Narrative Engine

The app's user experience is centered around a conversational interface designed to be supportive and clear, especially in high-stress situations.

6.1. Narrative Scripting with Ink

The Ink scripting language is the validated choice for authoring the Layer 2 branching conversation trees.

* Maturity and Expressiveness: Ink is an open-source language purpose-built for creating complex, branching narratives with state-tracking capabilities.
* Human-Readable: Scripts can be authored and audited by medical professionals without requiring deep programming knowledge.
* Platform Integration: It integrates well with native mobile apps, particularly on iOS via community wrappers like ink-iOS, which bridge the Ink JavaScript runtime with Swift.

6.2. User Interface and Experience

* UI Framework: MessageKit is recommended for iOS to build a scalable, chat-like interface. On Android, a custom implementation using RecyclerView or a commercial SDK like Stream Chat is proposed.
* Natural Flow: The design aims to feel less like a robotic menu and more like a conversation. This is achieved through techniques like using variables to recall user input ("I understand you're treating a burn on the arm...") and ensuring conversation state can be paused and resumed.

7. Development and Prototyping Roadmap

A phased approach is recommended, starting with critical de-risking activities before committing to full-scale development.

7.1. Critical Pre-Development Prototypes ("Spikes")

1. MLTextClassifier UX Validation: A "weekend spike" to build a prototype using MLTextClassifier. This will test the core thesis that a lightweight model can provide a "smart enough" user experience.
  * Success Criteria: >95% accuracy on a test set, <15% fallback rate to manual search, and positive user feedback on its intelligence.
2. Ink Conversation Tree Authoring Sprint: A one-week sprint to author and test a complete conversation tree for a single critical scenario (e.g., breathing difficulties).
  * Success Criteria: The script must be validated by emergency response professionals for medical accuracy and tested for a natural, non-robotic feel under simulated stress.
3. Content Pipeline Spike: Build a small-scale version of the Python script to process a ZIM file, chunk text, and generate embeddings.
  * Success Criteria: Validate the chunking strategy and confirm the high compression ratios achievable with Zstandard and custom dictionaries.

7.2. High-Level Success Metrics

Category	Metric	Target
Technical	Final App Size	< 480 MB (leaving buffer)
	Cold Start Launch Time	< 2 seconds
	Intent Classification Latency	< 500 ms (end-to-end)
	Hybrid Search Latency	< 2 seconds
	Battery Impact	< 1% additional drain per hour of active use
User Experience	Emergency Scenario Success Rate	>90% of users find critical info within 30 seconds
